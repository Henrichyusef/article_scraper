{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"webscraping.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMOLyL9JnBaIf1GOuhGS1PY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<font size=\"5\"> **WEB SCRAPER - LABMOL** </font> \n","\n","\n","<font size=\"2\"> by: Igor Sanches </font> "],"metadata":{"id":"qRVh7U177rOw"}},{"cell_type":"markdown","source":["**READ BEFORE CONTINUING!!**"],"metadata":{"id":"mEyNIZVGpEJl"}},{"cell_type":"markdown","source":["Please generate your api key at https://bit.ly/3dZC7VT (you will need it)\n","\n","**choose your keywords in the following format \"word1+word2+...+word5\"**\n","\n","choose a folder path to save the results file, see how: https://bit.ly/3sgLJUk\n"],"metadata":{"id":"JjlXywLWukO3"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"iTIaO0BD5w0O","cellView":"form","executionInfo":{"status":"ok","timestamp":1656633811132,"user_tz":180,"elapsed":114120,"user":{"displayName":"Igor Henrique","userId":"17041893385679009436"}}},"outputs":[],"source":["%%capture\n","#@title <-----  Install dependencies { vertical-output: true }\n","!pip install scholarly\n","!pip install requests\n","!pip install beautifulsoup4\n","!pip install pandas\n"]},{"cell_type":"code","source":["%%capture\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","###### add your folder path here ######\n","Save_Location = '/content/drive/MyDrive/' #type here the path where the results will be saved.\n","                                                      #do not add '/' at the end of the path."],"metadata":{"id":"i2rl4vi12gZY","executionInfo":{"status":"ok","timestamp":1656633837386,"user_tz":180,"elapsed":19436,"user":{"displayName":"Igor Henrique","userId":"17041893385679009436"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["#@title <- run\n","API_KEY = input(\"type your API key: \").strip()\n","keywords = input(\"keywords: \").strip().lower()\n","print(\"this will automatically save the results to a csv file at the chosen location\")\n","output = input(\"how do you wish to name the file? \").strip()\n","tables = []\n","\n","import requests\n","from bs4 import BeautifulSoup\n","import concurrent.futures\n","import csv\n","import urllib.parse\n","import pandas as pd\n","from scholarly import scholarly, ProxyGenerator\n","import re"],"metadata":{"id":"tIoWCJJb-ZMK","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1656633868769,"user_tz":180,"elapsed":30377,"user":{"displayName":"Igor Henrique","userId":"17041893385679009436"}},"outputId":"d4b1e704-3652-4afc-8bde-db9643d6b8ff"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["type your API key: jasdijasdihasluihasdijhasdasd\n","keywords: tomate+vermelho+genoma\n","this will automatically save the results to a csv file at the chosen location\n","how do you wish to name the file? primeira_busca\n"]}]},{"cell_type":"markdown","source":["# **GOOGLE SCHOLAR**\n","\n"],"metadata":{"id":"iH1gKotz6S5p"}},{"cell_type":"code","source":["#@title <- start { vertical-output: true }\n","\n","pages = 99\n","\n","pg = ProxyGenerator()\n","success = pg.ScraperAPI(API_KEY)\n","\n","#number of requests to be tried in case of failure \n","NUM_RETRIES = 3\n","\n","#number of threads to be executed concurrently\n","NUM_THREADS = 5\n","\n","#standard google scholar general search url\n","urlgo = f\"https://scholar.google.com/scholar?start=0&q={keywords}&hl=en&as_sdt=0,5\"\n","list_of_urls = []\n","\n","#generates gs urls based on the amount of pages the user requested\n","if pages == 1:\n","  list_of_urls = urlgo\n","else:\n","  for page in range(pages):\n","    list_of_urls.append(f'https://scholar.google.com/scholar?start={(page*10)}&q={keywords}&hl=en&as_sdt=0,5')\n","\n","#scraped data will be saved here\n","df0 = []\n","\n","def scrape_url(url):\n","    ##### SCRAPE FUNCTION  #####\n","      \n","    params = {'api_key': API_KEY, 'url': url}\n","    \n","    # send request to scraperapi, and automatically retry failed requests\n","    for _ in range(NUM_RETRIES):\n","        try:\n","            response = requests.get('http://api.scraperapi.com/', params = urllib.parse.urlencode(params))\n","            if response.status_code in [200, 404]:\n","                ## escape for loop if the API returns a successful response\n","                break\n","        except requests.exceptions.ConnectionError:\n","            response = ''\n","      \n","      \n","    ## parse data ONLY if 200 status code (successful response)\n","    if response.status_code == 200:\n","          \n","        ## parse syntax\n","        html_response = response.text\n","        soup = BeautifulSoup(html_response, \"html.parser\")\n","          \n","        #requesting title, link and year\n","        for item in soup.select('[data-lid]'):\n","          title = item.select(\"h3\")[0].get_text().replace('[HTML]', '')\n","          link = item.select(\"a\")[0][\"href\"]\n","          year = item.select(\".gs_a\")[0]\n","          yearstripped = year.get_text()\n","\n","          #retrives the year using regex\n","          finalyear = re.search(r\"(\\d{4})\", yearstripped).group()\n","          df0.append({\n","                'title': title,\n","                'link': link,\n","                'year': finalyear\n","            })\n","\n","#running multiple threads to speed up the scraping process  \n","with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n","    executor.map(scrape_url, list_of_urls)\n","\n","df0 = pd.DataFrame(df0)\n","print('DONE')\n","tables.append(df0)"],"metadata":{"id":"PU_45jZY5182","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **PUBMED**"],"metadata":{"id":"G-jIRw-a8mnh"}},{"cell_type":"code","source":["#@title <- start { vertical-output: true }\n","\n","pages = 99\n","\n","pg = ProxyGenerator()\n","success = pg.ScraperAPI(API_KEY)\n","\n","#number of requests to be tried in case of failure \n","NUM_RETRIES = 3\n","\n","#number of threads to be executed concurrently\n","NUM_THREADS = 5\n","\n","keywords = keywords.replace('+', '%20')\n","\n","#standard pubmed general search url\n","urlgo = f\"https://pubmed.ncbi.nlm.nih.gov/?term={keywords}\"\n","\n","\n","#generates gs urls based on the amount of pages the user requested\n","list_of_urls = []\n","if pages == 1:\n","  list_of_urls = urlgo\n","else:\n","  for page in range(pages):\n","    list_of_urls.append(f'https://pubmed.ncbi.nlm.nih.gov/?term={keywords}&page={page+1}')\n","\n","#scraped data will be saved here\n","df1 = []\n","\n","def scrape_url(url):\n","    ##### SCRAPE FUNCTION  #####\n","      \n","    params = {'api_key': API_KEY, 'url': url}\n","    \n","    # send request to scraperapi, and automatically retry failed requests\n","    for _ in range(NUM_RETRIES):\n","        try:\n","            response = requests.get('http://api.scraperapi.com/', params = urllib.parse.urlencode(params))\n","            if response.status_code in [200, 404]:\n","                ## escape for loop if the API returns a successful response\n","                break\n","        except requests.exceptions.ConnectionError:\n","            response = ''\n","      \n","      \n","    ## parse data ONLY if 200 status code (successful response)\n","    if response.status_code == 200:\n","          \n","        ## parse syntax\n","        html_response = response.text\n","        soup = BeautifulSoup(html_response, \"html.parser\")\n","          \n","        #requesting title, link and year\n","        for item in soup.select('[data-rel-pos]'):\n","          title = item.select(\"a\")[0].get_text().replace(\"\\n\", \"\").strip()\n","\n","          linknumber = item.select(\"a\")[0][\"href\"]\n","          link = f'https://pubmed.ncbi.nlm.nih.gov{linknumber}'\n","\n","          year = item.select(\".docsum-journal-citation.full-journal-citation\")[0]\n","          yearstripped = year.get_text()\n","\n","          #retrives the year using regex\n","          finalyear = re.search(r\"(\\d{4})\", yearstripped).group()\n","          df1.append({\n","                'title': title,\n","                'link': link,\n","                'year': finalyear\n","            })\n","          \n","\n","#running multiple threads to speed up the scraping process  \n","with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n","    executor.map(scrape_url, list_of_urls)\n","\n","df1 = pd.DataFrame(df1)\n","print('DONE')\n","tables.append(df1)"],"metadata":{"id":"SlmEo8sODghq","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **SEMANTICS SCHOLAR, SCIELO, WEB OF SCIENCE & SCIFINDER**\n","\n"],"metadata":{"id":"Qrnp4-SgLA30"}},{"cell_type":"code","source":["#@title <- start { vertical-output: true }\n","\n","pg = ProxyGenerator()\n","success = pg.ScraperAPI(API_KEY)\n","\n","#number of requests to be tried in case of failure \n","NUM_RETRIES = 3\n","\n","#number of threads to be executed concurrently\n","NUM_THREADS = 5\n","\n","#standard semantics scholar general search url\n","baseurl = f'https://api.semanticscholar.org/graph/v1/paper/search?query={keywords}&offset=0&limit=100&fields=title,year,url'\n","\n","#pre request to check the amount of papers to be scraped\n","r = requests.get(baseurl)\n","total = r.json()['total']\n","total = int(total)//100\n","list_of_urls = []\n","\n","#generates the list of urls\n","for i in range(total+1):\n","  list_of_urls.append(f'https://api.semanticscholar.org/graph/v1/paper/search?query={keywords}&offset={i*100}&limit=100&fields=title,year,url')\n","\n","\n","\n","#scraped data will be saved here\n","df2 = []\n","\n","def scrape_url(url):\n","    ##### SCRAPE FUNCTION  #####\n","      \n","    params = {'api_key': API_KEY, 'url': url}\n","    \n","    # send request to scraperapi, and automatically retry failed requests\n","    for _ in range(NUM_RETRIES):\n","        try:\n","            response = requests.get('http://api.scraperapi.com/', params = urllib.parse.urlencode(params))\n","            if response.status_code in [200, 404]:\n","                ## escape for loop if the API returns a successful response\n","                break\n","        except requests.exceptions.ConnectionError:\n","            response = ''\n","      \n","      \n","    ## parse data ONLY if 200 status code (successful response)\n","    if response.status_code == 200:\n","\n","      data = response.json()\n","      for i in data['data']:\n","        title = i['title']\n","        year = i['year']\n","        link = i['url']\n","\n","        df2.append({\n","          'title':title,\n","          'link':link,\n","          'year':year\n","        })\n","\n","#running multiple threads to speed up the scraping process  \n","with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n","    executor.map(scrape_url, list_of_urls)\n","\n","df2 = pd.DataFrame(df2)\n","print(\"DONE\")\n","tables.append(df2)"],"metadata":{"id":"8fHjAV_yQHyc","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=--=-=-=-=-=-=-=-=-=-=-=-=-=-"],"metadata":{"id":"hGrR3iuYjLY9"}},{"cell_type":"markdown","source":["# **SAVE AND EXPORT TO DRIVE**"],"metadata":{"id":"UP5ShXREhZG3"}},{"cell_type":"code","source":["#@title <-  run { vertical-output: true }\n","result = pd.concat(tables)\n","result.to_excel(f\"{Save_Location}/{output}.xlsx\", sheet_name= \"webscraping\")\n","print(\"DONE\")\n","pd.DataFrame(result)"],"metadata":{"id":"rI4oHiYziE2M","cellView":"form"},"execution_count":null,"outputs":[]}]}